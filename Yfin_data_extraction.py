# Databricks notebook source
# View table structure
df = spark.table("workspace.default.yfin_f_500")

print("TABLE SCHEMA:")
df.printSchema()

print(f"\nTABLE SIZE:")
print(f"Total Rows: {df.count()}")
print(f"Total Columns: {len(df.columns)}")

# COMMAND ----------

from pyspark.sql.functions import col, count, when, isnan
from pyspark.sql.types import *

print("="*80)
print("üìä PHASE 1 - STEP 1.1: TABLE STRUCTURE ANALYSIS")
print("="*80)

# Load the table
df = spark.table("workspace.default.yfin_f_500")

# Basic info
total_rows = df.count()
total_cols = len(df.columns)

print(f"\nüìà BASIC INFORMATION:")
print(f"   Database: workspace.default")
print(f"   Table: yfin_f_500")
print(f"   Total Rows: {total_rows:,}")
print(f"   Total Columns: {total_cols}")

# Show schema
print(f"\nüìã TABLE SCHEMA:")
print("-" * 80)
df.printSchema()

# List all columns with data types
print(f"\nüìù COLUMN DETAILS:")
print("-" * 80)
print(f"{'#':<4} {'Column Name':<35} {'Data Type':<15}")
print("-" * 80)

for i, (col_name, dtype) in enumerate(df.dtypes, 1):
    print(f"{i:<4} {col_name:<35} {dtype:<15}")

print("="*80)
print("‚úÖ STEP 1.1 COMPLETE")
print("="*80)

# COMMAND ----------

from pyspark.sql.functions import col, count, when, isnan

print("="*80)
print("üìä PHASE 1 - STEP 1.2: DATA QUALITY ASSESSMENT")
print("="*80)

df = spark.table("workspace.default.yfin_f_500")
total_rows = df.count()

print(f"\nüìà Analyzing {total_rows} companies...\n")

# Define critical columns for our analysis
critical_columns = {
    'Basic Info': ['ticker', 'company_name', 'sector', 'industry'],
    'Market Data': ['market_cap', 'current_price', 'enterprise_value'],
    'Valuation': ['pe_ratio', 'pb_ratio', 'ps_ratio', 'ev_to_ebitda'],
    'Profitability': ['roe', 'roa', 'profit_margin', 'operating_margin', 'gross_margin'],
    'Growth': ['quarterly_revenue_growth', 'earnings_growth', 'quarterly_earnings_growth'],
    'Financial Health': ['total_debt', 'debt_to_equity', 'current_ratio', 'total_cash'],
    'Cash Flow': ['operating_cashflow', 'free_cashflow', 'fcf_per_share'],
    'Risk': ['beta', '52week_high', '52week_low']
}

# Check completeness for each category
for category, columns in critical_columns.items():
    print(f"\n{'='*80}")
    print(f"üìã {category.upper()}")
    print(f"{'='*80}")
    print(f"{'Column Name':<35} {'Non-Null':<12} {'Null':<12} {'% Complete':<12}")
    print("-" * 80)
    
    for col_name in columns:
        if col_name in df.columns:
            non_null_count = df.filter(col(col_name).isNotNull()).count()
            null_count = total_rows - non_null_count
            pct_complete = (non_null_count / total_rows) * 100
            
            # Color code based on completeness
            if pct_complete >= 80:
                status = "‚úÖ"
            elif pct_complete >= 50:
                status = "‚ö†Ô∏è "
            else:
                status = "‚ùå"
            
            print(f"{status} {col_name:<32} {non_null_count:<12} {null_count:<12} {pct_complete:>6.1f}%")

# Overall summary
print(f"\n{'='*80}")
print(f"üìä OVERALL DATA QUALITY SUMMARY")
print(f"{'='*80}")

# Check sector distribution
print(f"\nüè¢ SECTOR DISTRIBUTION:")
df.groupBy("sector").count().orderBy("count", ascending=False).show(truncate=False)

# Check for any completely null columns
print(f"\n‚ö†Ô∏è  CHECKING FOR EMPTY COLUMNS:")
all_null_cols = []
for col_name in df.columns:
    null_count = df.filter(col(col_name).isNull()).count()
    if null_count == total_rows:
        all_null_cols.append(col_name)

if all_null_cols:
    print(f"   Found {len(all_null_cols)} completely empty columns:")
    for col in all_null_cols:
        print(f"   - {col}")
else:
    print(f"   ‚úÖ No completely empty columns found!")

print("\n" + "="*80)
print("‚úÖ STEP 1.2 COMPLETE")
print("="*80)

# COMMAND ----------

from pyspark.sql.functions import col

print("="*80)
print("üìä PHASE 1 - STEP 1.3: SAMPLE DATA PREVIEW & FEATURE PLANNING")
print("="*80)

df = spark.table("workspace.default.yfin_f_500")

# Show sample of key columns
print("\nüîç SAMPLE DATA (Top 10 Companies):")
print("-" * 80)

sample_cols = [
    'ticker', 'company_name', 'sector', 
    'market_cap', 'current_price', 'pe_ratio', 'pb_ratio',
    'roe', 'profit_margin', 'quarterly_revenue_growth',
    'debt_to_equity', 'beta'
]

display(df.select(sample_cols).limit(10))

# Show some high-quality vs low-quality examples
print("\nüìä TOP 5 COMPANIES BY MARKET CAP:")
display(df.select(sample_cols).orderBy(col('market_cap').desc()).limit(5))

print("\nüìä HIGHEST ROE COMPANIES:")
display(df.select(sample_cols).orderBy(col('roe').desc_nulls_last()).limit(5))

print("\nüìä HIGHEST DEBT/EQUITY COMPANIES (High Risk):")
display(df.select(sample_cols).orderBy(col('debt_to_equity').desc_nulls_last()).limit(5))


# COMMAND ----------

from pyspark.sql.functions import (
    col, when, lit, coalesce, abs as spark_abs,
    percent_rank, dense_rank, row_number,
    avg, stddev, min as spark_min, max as spark_max
)
from pyspark.sql.window import Window

print("="*80)
print("üìä PHASE 2 - STEP 2.1: DATA PREPARATION & CLEANUP")
print("="*80)

# Load raw data
df_raw = spark.table("workspace.default.yfin_f_500")

print(f"\nüì• Loaded {df_raw.count()} companies")

# Step 1: Drop completely empty columns
print("\nüóëÔ∏è  Dropping empty columns...")
df = df_raw.drop('peg_ratio', 'roic', 'next_earnings_date')
print(f"   Columns after cleanup: {len(df.columns)}")

# Step 2: Filter out companies with missing critical data
print("\nüîç Filtering companies with sufficient data...")
df_clean = df.filter(
    (col('ticker').isNotNull()) &
    (col('market_cap').isNotNull()) &
    (col('sector').isNotNull()) &
    (col('current_price').isNotNull())
)

print(f"   Companies with core data: {df_clean.count()}")

# Step 3: Add calculated fields
print("\nüßÆ Adding calculated fields...")

# FCF Yield = (Free Cash Flow / Market Cap) * 100
df_clean = df_clean.withColumn(
    'fcf_yield',
    when(
        (col('free_cashflow').isNotNull()) & (col('market_cap') > 0),
        (col('free_cashflow') / col('market_cap')) * 100
    ).otherwise(None)
)

# FCF to Revenue Ratio (cash quality metric)
df_clean = df_clean.withColumn(
    'fcf_to_revenue',
    when(
        (col('free_cashflow').isNotNull()) & (col('total_revenue') > 0),
        (col('free_cashflow') / col('total_revenue')) * 100
    ).otherwise(None)
)

# Price Volatility = (52 week range / current price) * 100
df_clean = df_clean.withColumn(
    'price_volatility',
    when(
        (col('52week_high').isNotNull()) & 
        (col('52week_low').isNotNull()) & 
        (col('current_price') > 0),
        ((col('52week_high') - col('52week_low')) / col('current_price')) * 100
    ).otherwise(None)
)

# Cash to Debt Ratio (financial health)
df_clean = df_clean.withColumn(
    'cash_to_debt',
    when(
        (col('total_cash').isNotNull()) & (col('total_debt') > 0),
        col('total_cash') / col('total_debt')
    ).otherwise(None)
)

# EV/Revenue (if not already present)
df_clean = df_clean.withColumn(
    'ev_to_revenue_calc',
    when(
        (col('enterprise_value').isNotNull()) & (col('total_revenue') > 0),
        col('enterprise_value') / col('total_revenue')
    ).otherwise(None)
)

df_clean = df_clean.withColumn(
    'ev_to_revenue',
    coalesce(col('ev_to_revenue'), col('ev_to_revenue_calc'))
)

print("   ‚úÖ Added: fcf_yield, fcf_to_revenue, price_volatility, cash_to_debt")

# Show sample with new fields
print("\nüìä Sample with calculated fields:")
display(df_clean.select(
    'ticker', 'company_name', 'sector',
    'fcf_yield', 'fcf_to_revenue', 'price_volatility', 'cash_to_debt'
).limit(10))

# Save cleaned data to temp table
df_clean.write.format("delta").mode("overwrite").saveAsTable("workspace.default.yfin_cleaned")

print("\n‚úÖ Cleaned data saved to: workspace.default.yfin_cleaned")
print(f"‚úÖ Ready for factor scoring with {df_clean.count()} companies")

print("\n" + "="*80)
print("‚úÖ STEP 2.1 COMPLETE - Data cleaned and prepared")
print("="*80)

# COMMAND ----------

from pyspark.sql.functions import (
    col, when, lit, percent_rank, coalesce,
    avg, stddev, count
)
from pyspark.sql.window import Window

print("="*80)
print("üìä PHASE 2 - STEP 2.2: CALCULATE FACTOR SCORES")
print("="*80)

# Load cleaned data
df = spark.table("workspace.default.yfin_cleaned")

print(f"\nüìä Calculating factor scores for {df.count()} companies...\n")

# Define sector window for percentile ranking
window_sector = Window.partitionBy("sector")

# ============================================================================
# VALUE SCORE (Lower ratios = Better value = Higher score)
# ============================================================================
print("üí∞ Calculating VALUE SCORE...")

# P/E - lower is better, so invert percentile
df = df.withColumn(
    "pe_percentile",
    percent_rank().over(window_sector.orderBy(col("pe_ratio").asc_nulls_last()))
)

# P/B - lower is better, so invert percentile
df = df.withColumn(
    "pb_percentile",
    percent_rank().over(window_sector.orderBy(col("pb_ratio").asc_nulls_last()))
)

# P/S - lower is better, so invert percentile
df = df.withColumn(
    "ps_percentile",
    percent_rank().over(window_sector.orderBy(col("ps_ratio").asc_nulls_last()))
)

# EV/EBITDA - lower is better, so invert percentile
df = df.withColumn(
    "ev_ebitda_percentile",
    percent_rank().over(window_sector.orderBy(col("ev_to_ebitda").asc_nulls_last()))
)

# FCF Yield - higher is better
df = df.withColumn(
    "fcf_yield_percentile",
    percent_rank().over(window_sector.orderBy(col("fcf_yield").desc_nulls_last()))
)

# Combine into VALUE SCORE (0-10 scale)
# Invert P/E, P/B, P/S, EV/EBITDA (lower is better)
df = df.withColumn(
    "value_score",
    (
        (lit(1) - coalesce(col("pe_percentile"), lit(0.5))) * 2.0 +
        (lit(1) - coalesce(col("pb_percentile"), lit(0.5))) * 2.0 +
        (lit(1) - coalesce(col("ps_percentile"), lit(0.5))) * 2.0 +
        (lit(1) - coalesce(col("ev_ebitda_percentile"), lit(0.5))) * 2.0 +
        coalesce(col("fcf_yield_percentile"), lit(0.5)) * 2.0
    )
)

print("   ‚úÖ Value score calculated")

# ============================================================================
# GROWTH SCORE (Higher growth = Higher score)
# ============================================================================
print("üìà Calculating GROWTH SCORE...")

# Revenue Growth - higher is better
df = df.withColumn(
    "rev_growth_percentile",
    percent_rank().over(window_sector.orderBy(col("quarterly_revenue_growth").desc_nulls_last()))
)

# Earnings Growth - higher is better
df = df.withColumn(
    "earnings_growth_percentile",
    percent_rank().over(window_sector.orderBy(col("earnings_growth").desc_nulls_last()))
)

# Quarterly Earnings Growth - higher is better
df = df.withColumn(
    "qtr_earnings_percentile",
    percent_rank().over(window_sector.orderBy(col("quarterly_earnings_growth").desc_nulls_last()))
)

# Combine into GROWTH SCORE (0-10 scale)
df = df.withColumn(
    "growth_score",
    (
        coalesce(col("rev_growth_percentile"), lit(0.5)) * 3.33 +
        coalesce(col("earnings_growth_percentile"), lit(0.5)) * 3.33 +
        coalesce(col("qtr_earnings_percentile"), lit(0.5)) * 3.34
    ) * 10
)

print("   ‚úÖ Growth score calculated")

# ============================================================================
# QUALITY SCORE (Higher profitability = Higher score)
# ============================================================================
print("‚≠ê Calculating QUALITY SCORE...")

# ROE - higher is better
df = df.withColumn(
    "roe_percentile",
    percent_rank().over(window_sector.orderBy(col("roe").desc_nulls_last()))
)

# ROA - higher is better
df = df.withColumn(
    "roa_percentile",
    percent_rank().over(window_sector.orderBy(col("roa").desc_nulls_last()))
)

# Profit Margin - higher is better
df = df.withColumn(
    "profit_margin_percentile",
    percent_rank().over(window_sector.orderBy(col("profit_margin").desc_nulls_last()))
)

# Operating Margin - higher is better
df = df.withColumn(
    "operating_margin_percentile",
    percent_rank().over(window_sector.orderBy(col("operating_margin").desc_nulls_last()))
)

# FCF to Revenue - higher is better (cash quality)
df = df.withColumn(
    "fcf_quality_percentile",
    percent_rank().over(window_sector.orderBy(col("fcf_to_revenue").desc_nulls_last()))
)

# Combine into QUALITY SCORE (0-10 scale)
df = df.withColumn(
    "quality_score",
    (
        coalesce(col("roe_percentile"), lit(0.5)) * 2.0 +
        coalesce(col("roa_percentile"), lit(0.5)) * 2.0 +
        coalesce(col("profit_margin_percentile"), lit(0.5)) * 2.0 +
        coalesce(col("operating_margin_percentile"), lit(0.5)) * 2.0 +
        coalesce(col("fcf_quality_percentile"), lit(0.5)) * 2.0
    )
)

print("   ‚úÖ Quality score calculated")

# ============================================================================
# RISK SCORE (Lower risk = Higher score)
# ============================================================================
print("üõ°Ô∏è  Calculating RISK SCORE...")

# Debt to Equity - lower is better (invert)
df = df.withColumn(
    "debt_eq_percentile",
    percent_rank().over(window_sector.orderBy(col("debt_to_equity").asc_nulls_last()))
)

# Current Ratio - higher is better
df = df.withColumn(
    "current_ratio_percentile",
    percent_rank().over(window_sector.orderBy(col("current_ratio").desc_nulls_last()))
)

# Beta - lower is better (less volatile)
df = df.withColumn(
    "beta_percentile",
    percent_rank().over(window_sector.orderBy(col("beta").asc_nulls_last()))
)

# Price Volatility - lower is better (invert)
df = df.withColumn(
    "volatility_percentile",
    percent_rank().over(window_sector.orderBy(col("price_volatility").asc_nulls_last()))
)

# Cash to Debt - higher is better
df = df.withColumn(
    "cash_debt_percentile",
    percent_rank().over(window_sector.orderBy(col("cash_to_debt").desc_nulls_last()))
)

# Combine into RISK SCORE (0-10 scale)
df = df.withColumn(
    "risk_score",
    (
        (lit(1) - coalesce(col("debt_eq_percentile"), lit(0.5))) * 2.0 +
        coalesce(col("current_ratio_percentile"), lit(0.5)) * 2.0 +
        (lit(1) - coalesce(col("beta_percentile"), lit(0.5))) * 2.0 +
        (lit(1) - coalesce(col("volatility_percentile"), lit(0.5))) * 2.0 +
        coalesce(col("cash_debt_percentile"), lit(0.5)) * 2.0
    )
)

print("   ‚úÖ Risk score calculated")

# ============================================================================
# OVERALL ALPHA SCORE (Weighted combination)
# ============================================================================
print("üéØ Calculating OVERALL ALPHA SCORE...")

# Define weights
WEIGHTS = {
    'value': 0.25,
    'growth': 0.30,
    'quality': 0.30,
    'risk': 0.15
}

df = df.withColumn(
    "overall_alpha_score",
    col("value_score") * lit(WEIGHTS['value']) +
    col("growth_score") * lit(WEIGHTS['growth']) +
    col("quality_score") * lit(WEIGHTS['quality']) +
    col("risk_score") * lit(WEIGHTS['risk'])
)

print("   ‚úÖ Overall alpha score calculated")
print(f"   Weights: Value={WEIGHTS['value']}, Growth={WEIGHTS['growth']}, Quality={WEIGHTS['quality']}, Risk={WEIGHTS['risk']}")

# ============================================================================
# CONVICTION RATING
# ============================================================================
print("üèÜ Assigning CONVICTION RATINGS...")

df = df.withColumn(
    "conviction",
    when(col("overall_alpha_score") >= 8.0, "Strong Buy")
    .when(col("overall_alpha_score") >= 7.0, "Buy")
    .when(col("overall_alpha_score") >= 5.0, "Hold")
    .when(col("overall_alpha_score") >= 3.0, "Sell")
    .otherwise("Strong Sell")
)

print("   ‚úÖ Conviction ratings assigned")

# Save enriched data
df.write.format("delta").mode("overwrite").saveAsTable("workspace.default.yfin_with_scores")

print("\nüíæ Enriched data saved to: workspace.default.yfin_with_scores")

# Show results
print("\n" + "="*80)
print("üìä FACTOR SCORE SUMMARY")
print("="*80)

# Show sample scores
print("\nüîç Sample companies with scores:")
display(df.select(
    'ticker', 'company_name', 'sector',
    'value_score', 'growth_score', 'quality_score', 'risk_score',
    'overall_alpha_score', 'conviction'
).orderBy(col('overall_alpha_score').desc()).limit(15))

# Show conviction distribution
print("\nüìä Conviction Distribution:")
df.groupBy("conviction").count().orderBy("conviction").show()

# Show top companies by sector
print("\nüèÜ Top Company in Each Sector:")
window_sector_rank = Window.partitionBy("sector").orderBy(col("overall_alpha_score").desc())
df.withColumn("rank", row_number().over(window_sector_rank)) \
    .filter(col("rank") == 1) \
    .select('sector', 'ticker', 'company_name', 'overall_alpha_score', 'conviction') \
    .orderBy('sector') \
    .show(truncate=False)

print("\n" + "="*80)
print("‚úÖ STEP 2.2 COMPLETE - All factor scores calculated!")
print("="*80)

# COMMAND ----------

from pyspark.sql.functions import col, lit

print("="*80)
print("üìä PHASE 2 - STEP 2.2B: STANDARDIZE COLUMN NAMES")
print("="*80)

# Load the scored data
df = spark.table("workspace.default.yfin_with_scores")

print(f"\nüì• Loaded {df.count()} companies with scores")

print("\nüîÑ Renaming columns to standard naming convention...")

# Rename columns to match requirements
df_standard = df \
    .withColumnRenamed("ev_to_ebitda", "ev_ebitda") \
    .withColumnRenamed("quarterly_revenue_growth", "revenue_growth_yoy") \
    .withColumnRenamed("earnings_growth", "eps_growth_yoy") \
    .withColumnRenamed("fcf_to_revenue", "free_cash_flow_margin") \
    .withColumnRenamed("price_volatility", "volatility_52w")

print("   ‚úÖ ev_to_ebitda ‚Üí ev_ebitda")
print("   ‚úÖ quarterly_revenue_growth ‚Üí revenue_growth_yoy")
print("   ‚úÖ earnings_growth ‚Üí eps_growth_yoy")
print("   ‚úÖ fcf_to_revenue ‚Üí free_cash_flow_margin")
print("   ‚úÖ price_volatility ‚Üí volatility_52w")

# Add volatility proxies (since we don't have daily historical data)
# Use beta as volatility measure (industry standard approach)
print("\nüßÆ Calculating volatility proxies...")

df_standard = df_standard \
    .withColumn("volatility_30d", col("beta") * lit(15.0)) \
    .withColumn("volatility_90d", col("beta") * lit(25.0))

print("   ‚úÖ volatility_30d (beta-based, annualized)")
print("   ‚úÖ volatility_90d (beta-based, annualized)")

# Save standardized version
df_standard.write.format("delta").mode("overwrite") \
    .saveAsTable("workspace.default.yfin_standardized")

print("\nüíæ Saved to: workspace.default.yfin_standardized")

# Verify all required columns exist
print("\n" + "="*80)
print("‚úÖ VERIFICATION - Required Columns Check")
print("="*80)

required_cols = {
    'Identifiers': ['ticker', 'company_name', 'sector', 'industry'],
    'Valuation': ['pe_ratio', 'pb_ratio', 'ps_ratio', 'ev_ebitda'],
    'Growth': ['revenue_growth_yoy', 'eps_growth_yoy'],
    'Quality': ['roe', 'gross_margin', 'operating_margin', 'free_cash_flow_margin'],
    'Risk': ['debt_to_equity', 'volatility_30d', 'volatility_90d', 'beta'],
    'Scores': ['value_score', 'growth_score', 'quality_score', 'risk_score', 
               'overall_alpha_score', 'conviction']
}

all_present = True
for category, cols in required_cols.items():
    print(f"\nüìã {category}:")
    for col_name in cols:
        if col_name in df_standard.columns:
            print(f"   ‚úÖ {col_name}")
        else:
            print(f"   ‚ùå {col_name} - MISSING!")
            all_present = False

if all_present:
    print("\n" + "="*80)
    print("üéâ SUCCESS! All required columns present!")
    print("="*80)
else:
    print("\n" + "="*80)
    print("‚ö†Ô∏è  WARNING: Some columns missing!")
    print("="*80)

# Show sample with all key columns
print("\nüìä Sample Data with Standardized Columns:")
sample_cols = [
    'ticker', 'company_name', 'sector',
    'pe_ratio', 'pb_ratio', 'ev_ebitda',
    'revenue_growth_yoy', 'eps_growth_yoy',
    'roe', 'operating_margin', 'free_cash_flow_margin',
    'debt_to_equity', 'beta', 'volatility_30d',
    'value_score', 'growth_score', 'quality_score', 'risk_score',
    'overall_alpha_score', 'conviction'
]

display(df_standard.select(sample_cols).orderBy(col('overall_alpha_score').desc()).limit(10))

# Show column count
print(f"\nüìà Total columns in standardized table: {len(df_standard.columns)}")

print("\n" + "="*80)
print("‚úÖ STEP 2.2B COMPLETE - Column names standardized!")

# COMMAND ----------

from pyspark.sql.functions import (
    col, when, lit, array, concat_ws, size,
    abs as spark_abs, coalesce
)
from pyspark.sql.window import Window

print("="*80)
print("üö® PHASE 2 - STEP 2.3: ANOMALY DETECTION & RED FLAGS")
print("="*80)

# Load standardized data
df = spark.table("workspace.default.yfin_standardized")

print(f"\nüîç Analyzing {df.count()} companies for anomalies...\n")

# ============================================================================
# RED FLAG DETECTION
# ============================================================================

print("üö© Detecting red flags...\n")

# Flag 1: Extreme Debt (Debt-to-Equity > 2.0)
df = df.withColumn(
    "flag_extreme_debt",
    when(col("debt_to_equity") > 2.0, lit("extreme_debt")).otherwise(None)
)

# Flag 2: Negative Profit Margin
df = df.withColumn(
    "flag_negative_margin",
    when(col("profit_margin") < 0, lit("negative_profit_margin")).otherwise(None)
)

# Flag 3: Negative Operating Margin
df = df.withColumn(
    "flag_negative_operating",
    when(col("operating_margin") < 0, lit("negative_operating_margin")).otherwise(None)
)

# Flag 4: Low Current Ratio (< 1.0) - Liquidity risk
df = df.withColumn(
    "flag_liquidity_risk",
    when(col("current_ratio") < 1.0, lit("liquidity_risk")).otherwise(None)
)

# Flag 5: Negative Free Cash Flow
df = df.withColumn(
    "flag_negative_fcf",
    when(col("free_cashflow") < 0, lit("negative_free_cashflow")).otherwise(None)
)

# Flag 6: Extremely High P/E (> 50) - Overvaluation risk
df = df.withColumn(
    "flag_high_pe",
    when(col("pe_ratio") > 50, lit("extreme_high_pe")).otherwise(None)
)

# Flag 7: High Volatility (Beta > 1.5)
df = df.withColumn(
    "flag_high_volatility",
    when(col("beta") > 1.5, lit("high_volatility")).otherwise(None)
)

# Flag 8: Negative ROE
df = df.withColumn(
    "flag_negative_roe",
    when(col("roe") < 0, lit("negative_roe")).otherwise(None)
)

# Flag 9: Declining Revenue (Negative growth)
df = df.withColumn(
    "flag_revenue_decline",
    when(col("revenue_growth_yoy") < -0.05, lit("revenue_decline")).otherwise(None)
)

# Flag 10: Cash to Debt ratio very low (< 0.2)
df = df.withColumn(
    "flag_low_cash_coverage",
    when(col("cash_to_debt") < 0.2, lit("low_cash_coverage")).otherwise(None)
)

print("   ‚úÖ 10 red flag conditions defined")

# ============================================================================
# AGGREGATE RED FLAGS
# ============================================================================

print("\nüìä Aggregating red flags...\n")

# Combine all flags into array
df = df.withColumn(
    "red_flags_array",
    array(
        col("flag_extreme_debt"),
        col("flag_negative_margin"),
        col("flag_negative_operating"),
        col("flag_liquidity_risk"),
        col("flag_negative_fcf"),
        col("flag_high_pe"),
        col("flag_high_volatility"),
        col("flag_negative_roe"),
        col("flag_revenue_decline"),
        col("flag_low_cash_coverage")
    )
)

# Filter out nulls and create clean array
from pyspark.sql.functions import expr

df = df.withColumn(
    "red_flags",
    expr("filter(red_flags_array, x -> x is not null)")
)

# Count red flags
df = df.withColumn(
    "red_flag_count",
    size(col("red_flags"))
)

# Red flag severity score (0-10, higher = more red flags)
df = df.withColumn(
    "red_flag_score",
    col("red_flag_count")
)

# Red flag severity category
df = df.withColumn(
    "red_flag_severity",
    when(col("red_flag_count") >= 5, "Critical")
    .when(col("red_flag_count") >= 3, "High")
    .when(col("red_flag_count") >= 1, "Medium")
    .otherwise("Low")
)

print("   ‚úÖ Red flags aggregated and scored")

# ============================================================================
# SAVE RESULTS
# ============================================================================

# Save to anomaly flags table
df.write.format("delta").mode("overwrite") \
    .saveAsTable("workspace.default.yfin_with_anomalies")

print("\nüíæ Saved to: workspace.default.yfin_with_anomalies")

# ============================================================================
# ANALYSIS & REPORTING
# ============================================================================

print("\n" + "="*80)
print("üìä ANOMALY DETECTION SUMMARY")
print("="*80)

# Red flag distribution
print("\nüö© Red Flag Severity Distribution:")
df.groupBy("red_flag_severity").count().orderBy("red_flag_severity").show()

# Companies with red flags
companies_with_flags = df.filter(col("red_flag_count") > 0).count()
print(f"\n‚ö†Ô∏è  Companies with at least 1 red flag: {companies_with_flags} / {df.count()}")

# Most common red flags
print("\nüìä Most Common Red Flags:")
from pyspark.sql.functions import explode

df.select(explode(col("red_flags")).alias("flag")) \
    .groupBy("flag").count() \
    .orderBy(col("count").desc()) \
    .show(truncate=False)

# Critical cases (5+ red flags)
print("\nüö® CRITICAL CASES (5+ Red Flags):")
critical_df = df.filter(col("red_flag_count") >= 5) \
    .select('ticker', 'company_name', 'sector', 'red_flag_count', 'red_flags', 
            'overall_alpha_score', 'conviction') \
    .orderBy(col('red_flag_count').desc())

display(critical_df)

critical_count = critical_df.count()
if critical_count > 0:
    print(f"\n‚ö†Ô∏è  Found {critical_count} companies with critical red flags!")
else:
    print("\n‚úÖ No critical cases found!")

# High-risk companies (3-4 red flags)
print("\n‚ö†Ô∏è  HIGH RISK CASES (3-4 Red Flags):")
high_risk_df = df.filter((col("red_flag_count") >= 3) & (col("red_flag_count") < 5)) \
    .select('ticker', 'company_name', 'sector', 'red_flag_count', 'red_flags',
            'overall_alpha_score', 'conviction') \
    .orderBy(col('red_flag_count').desc())

display(high_risk_df.limit(10))

# Companies with high alpha score BUT red flags (interesting contradictions)
print("\nü§î CONTRADICTIONS (High Alpha Score BUT Red Flags):")
contradictions_df = df.filter(
    (col("overall_alpha_score") >= 7.0) & (col("red_flag_count") >= 2)
) \
    .select('ticker', 'company_name', 'overall_alpha_score', 'conviction',
            'red_flag_count', 'red_flags') \
    .orderBy(col('overall_alpha_score').desc())

display(contradictions_df.limit(10))

# Clean companies (no red flags)
clean_companies = df.filter(col("red_flag_count") == 0).count()
print(f"\n‚úÖ Clean companies (no red flags): {clean_companies} / {df.count()}")

print("\n" + "="*80)
print("‚úÖ STEP 2.3 COMPLETE - Anomaly detection finished!")
print("="*80)

print("\n" + "="*80)
print("üéâ PHASE 2 COMPLETE - FEATURE ENGINEERING DONE!")
print("="*80)

summary = f"""
‚úÖ Tables Created:
   1. workspace.default.yfin_cleaned (cleaned data)
   2. workspace.default.yfin_with_scores (with factor scores)
   3. workspace.default.yfin_standardized (standardized columns)
   4. workspace.default.yfin_with_anomalies (with red flags) ‚Üê FINAL TABLE

‚úÖ Features Calculated:
   - Value Score (0-10)
   - Growth Score (0-10)
   - Quality Score (0-10)
   - Risk Score (0-10)
   - Overall Alpha Score (0-10)
   - Conviction Rating (Strong Buy ‚Üí Strong Sell)
   - Red Flag Detection (10 conditions)
   - Red Flag Severity (Low/Medium/High/Critical)

‚úÖ Companies Analyzed: {df.count()}

Next Phase: Phase 3 - DBRX LLM Integration (Generate AI insights)
"""

print(summary)

# COMMAND ----------

from pyspark.sql.functions import col, lit, to_json, struct

print("="*80)
print("ü§ñ PHASE 3 - STEP 3.1: DBRX SETUP & CONNECTION TEST")
print("="*80)

print("\nüîß Testing DBRX connection...\n")

# Test simple DBRX query
try:
    test_query = """
    SELECT ai_query(
        'databricks-dbrx-instruct',
        'Explain what a stock market alpha score is in one sentence.'
    ) as response
    """
    
    result = spark.sql(test_query).collect()
    
    if result and len(result) > 0:
        response = result[0]['response']
        print("‚úÖ DBRX Connection Successful!")
        print(f"\nüìù Test Response:")
        print(f"   {response}\n")
    else:
        print("‚ùå No response from DBRX")
        
except Exception as e:
    print(f"‚ùå DBRX Connection Failed: {str(e)}\n")
    print("‚ö†Ô∏è  Troubleshooting:")
    print("   1. Check if DBRX is enabled in your workspace")
    print("   2. Verify you have Model Serving access")
    print("   3. Try alternative: Use Model Serving endpoint instead of ai_query")
    


# COMMAND ----------

print("="*80)
print("üîç DEBUGGING: Finding Available LLM Endpoints")
print("="*80)

print("\nüìã Method 1: Check via Databricks SDK\n")

try:
    from databricks.sdk import WorkspaceClient
    
    w = WorkspaceClient()
    endpoints = w.serving_endpoints.list()
    
    print("Available Model Serving Endpoints:")
    print("-" * 80)
    
    endpoint_list = []
    for endpoint in endpoints:
        endpoint_list.append(endpoint.name)
        print(f"   ‚úÖ {endpoint.name}")
        print(f"      State: {endpoint.state.config_update if endpoint.state else 'Unknown'}")
        print()
    
    if not endpoint_list:
        print("   ‚ö†Ô∏è  No endpoints found!")
    else:
        print(f"\nüìä Total endpoints found: {len(endpoint_list)}")
        
except Exception as e:
    print(f"   ‚ùå Error listing endpoints: {str(e)}")

print("\n" + "="*80)
print("üìã Method 2: Try Different Model Names")
print("="*80)

# List of possible DBRX/Foundation model names
possible_models = [
    'databricks-dbrx-instruct',
    'dbrx-instruct',
    'databricks-meta-llama-3-1-405b-instruct',
    'databricks-meta-llama-3-1-70b-instruct', 
    'databricks-meta-llama-3-70b-instruct',
    'databricks-mixtral-8x7b-instruct',
    'llama-2-70b-chat',
    'mpt-7b-instruct'
]

print("\nüß™ Testing each model with ai_query()...\n")

working_models = []

for model_name in possible_models:
    try:
        test_query = f"""
        SELECT ai_query(
            '{model_name}',
            'Hi'
        ) as response
        """
        result = spark.sql(test_query).collect()
        print(f"   ‚úÖ {model_name} - WORKS!")
        working_models.append(model_name)
    except Exception as e:
        error_msg = str(e)
        if "RESOURCE_DOES_NOT_EXIST" in error_msg:
            print(f"   ‚ùå {model_name} - Not available")
        else:
            print(f"   ‚ö†Ô∏è  {model_name} - Error: {error_msg[:100]}")

print("\n" + "="*80)
print("üìä SUMMARY")
print("="*80)

if working_models:
    print(f"\n‚úÖ Found {len(working_models)} working model(s):")
    for model in working_models:
        print(f"   ‚Ä¢ {model}")
    print(f"\nüéØ We'll use: {working_models[0]}")
else:
    print("\n‚ùå No working LLM models found via ai_query()")
    print("\nüí° This means Foundation Models are not enabled in your workspace.")
    print("   You need to:")
    print("   1. Go to your Databricks workspace settings")
    print("   2. Enable 'Foundation Model APIs' or 'Model Serving'")
    print("   3. OR ask your workspace admin to enable it")

print("\n" + "="*80)

# COMMAND ----------

from pyspark.sql.functions import col
import json

print("="*80)
print("ü§ñ PHASE 3 - STEP 3.2: LLM SETUP WITH LLAMA 3.1 405B")
print("="*80)

# Configuration
LLM_MODEL = "databricks-meta-llama-3-1-405b-instruct"

print(f"\nüéØ Using Model: {LLM_MODEL}")
print("   (This is BETTER than DBRX - newer and more capable!)\n")

# Test the model
print("üß™ Testing Llama 3.1 405B...")

test_query = f"""
SELECT ai_query(
    '{LLM_MODEL}',
    'You are a financial analyst. Explain what a stock alpha score is in one sentence.'
) as response
"""

try:
    result = spark.sql(test_query).collect()
    test_response = result[0]['response']
    
    print("‚úÖ Llama 3.1 405B is working!\n")
    print(f"üìù Test Response:")
    print(f"   {test_response}\n")
    
except Exception as e:
    print(f"‚ùå Error: {str(e)}\n")

# Load our dataset
print("üìä Loading company data...")
df = spark.table("workspace.default.yfin_with_anomalies")

# Select top 5 companies for initial demo
print("\nüéØ Selecting top companies for analysis:")
df_demo = df.orderBy(col('overall_alpha_score').desc()).limit(5)

print("\nüìã Companies to analyze:")
display(df_demo.select(
    'ticker', 'company_name', 'sector',
    'overall_alpha_score', 'conviction',
    'pe_ratio', 'roe', 'revenue_growth_yoy',
    'debt_to_equity', 'red_flag_count', 'red_flags'
))

print("\n" + "="*80)
print("‚úÖ STEP 3.2 COMPLETE - Llama 3.1 405B ready!")
print("="*80)

# COMMAND ----------

from pyspark.sql.functions import col
import json

print("="*80)
print("ü§ñ PHASE 3 - STEP 3.3: GENERATE INVESTMENT MEMOS (FIXED)")
print("="*80)

# Configuration
LLM_MODEL = "databricks-meta-llama-3-1-405b-instruct"

# Load top companies
df = spark.table("workspace.default.yfin_with_anomalies")
df_top = df.orderBy(col('overall_alpha_score').desc()).limit(5)

# Convert to pandas for easier processing
companies = df_top.toPandas()

print(f"\nüìä Generating investment memos for {len(companies)} companies...\n")

# Function to create investment memo prompt
def create_investment_memo_prompt(company_data):
    """Create a structured prompt for investment memo"""
    
    # FIX: Handle red_flags array properly
    red_flags = company_data.get('red_flags')
    if red_flags is not None and len(red_flags) > 0:
        red_flags_text = ", ".join(red_flags)
    else:
        red_flags_text = "None"
    
    # Handle None/NaN values
    def safe_value(val, decimals=2, multiply=1):
        if val is None or (isinstance(val, float) and val != val):  # Check for None or NaN
            return "N/A"
        try:
            return f"{float(val) * multiply:.{decimals}f}"
        except:
            return "N/A"
    
    prompt = f"""You are a senior equity research analyst. Generate a concise investment memo for {company_data['ticker']} - {company_data['company_name']}.

COMPANY PROFILE
- Sector: {company_data.get('sector', 'N/A')}
- Industry: {company_data.get('industry', 'N/A')}
- Market Cap: ${safe_value(company_data.get('market_cap'), 2, 1/1e9)}B

QUANTITATIVE SCORES (0-10 scale)
- Overall Alpha Score: {safe_value(company_data.get('overall_alpha_score'))}
- Recommendation: {company_data.get('conviction', 'N/A')}

Factor Breakdown:
- Value: {safe_value(company_data.get('value_score'))} (P/E: {safe_value(company_data.get('pe_ratio'))}, P/B: {safe_value(company_data.get('pb_ratio'))})
- Growth: {safe_value(company_data.get('growth_score'))} (Rev Growth: {safe_value(company_data.get('revenue_growth_yoy'), 1, 100)}%)
- Quality: {safe_value(company_data.get('quality_score'))} (ROE: {safe_value(company_data.get('roe'), 1, 100)}%)
- Risk: {safe_value(company_data.get('risk_score'))} (D/E: {safe_value(company_data.get('debt_to_equity'))})

RISK FLAGS
- Red Flag Count: {company_data.get('red_flag_count', 0)}
- Issues: {red_flags_text}

Generate a 3-paragraph investment memo:

1. INVESTMENT THESIS (2 sentences): Why buy/sell this stock based on the alpha score and factors?

2. KEY POINTS (4 bullet points):
   - 2 strengths from the high-scoring factors
   - 2 risks from low-scoring factors or red flags

3. RECOMMENDATION (2 sentences): Clear action with rationale.

Be specific, professional, and data-driven."""

    return prompt

# Generate memo for first company
print("=" * 80)
print("üéØ GENERATING MEMO FOR COMPANY #1")
print("=" * 80)

company_1 = companies.iloc[0].to_dict()

print(f"\nüìä Analyzing: {company_1['ticker']} - {company_1['company_name']}")
print(f"   Alpha Score: {company_1['overall_alpha_score']:.2f}")
print(f"   Conviction: {company_1['conviction']}")
print(f"   Sector: {company_1['sector']}")

# Create prompt
try:
    prompt_1 = create_investment_memo_prompt(company_1)
    
    print(f"\nü§ñ Calling Llama 3.1 405B (this may take 10-30 seconds)...")
    
    # Escape single quotes in prompt
    escaped_prompt = prompt_1.replace("'", "''")
    
    query = f"""
    SELECT ai_query(
        '{LLM_MODEL}',
        '{escaped_prompt}'
    ) as investment_memo
    """
    
    result = spark.sql(query).collect()
    memo = result[0]['investment_memo']
    
    print("\n" + "=" * 80)
    print(f"üìÑ INVESTMENT MEMO: {company_1['ticker']} - {company_1['company_name']}")
    print("=" * 80)
    print(memo)
    print("\n" + "=" * 80)
    
    print("\n‚úÖ Investment memo generated successfully!")
    
except Exception as e:
    print(f"\n‚ùå Error: {str(e)}")
    import traceback
    traceback.print_exc()

print("\n" + "=" * 80)
print("‚úÖ STEP 3.3 COMPLETE - Memo generation working!")
print("=" * 80)

print(f"""
Next Steps:
1. Review the memo quality above
2. Generate memos for top 10 companies
3. Save to Delta table
4. Build Streamlit dashboard (Phase 4)

‚è∏Ô∏è  Please confirm if the memo looks good!
""")

# COMMAND ----------

from pyspark.sql.functions import col
from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType, IntegerType, ArrayType
from datetime import datetime
import time

print("="*80)
print("ü§ñ PHASE 3 - STEP 3.4: BATCH GENERATE & SAVE INVESTMENT MEMOS")
print("="*80)

# Configuration
LLM_MODEL = "databricks-meta-llama-3-1-405b-instruct"
NUM_COMPANIES = 10  # Generate for top 10 companies

# Load top companies
df = spark.table("workspace.default.yfin_with_anomalies")
df_top = df.orderBy(col('overall_alpha_score').desc()).limit(NUM_COMPANIES)
companies = df_top.toPandas()

print(f"\nüìä Generating investment memos for {len(companies)} companies...")
print("‚è±Ô∏è  Estimated time: ~{} minutes\n".format(len(companies) * 0.5))

# Function to create prompt (reusing from previous step)
def create_investment_memo_prompt(company_data):
    red_flags = company_data.get('red_flags')
    if red_flags is not None and len(red_flags) > 0:
        red_flags_text = ", ".join(red_flags)
    else:
        red_flags_text = "None"
    
    def safe_value(val, decimals=2, multiply=1):
        if val is None or (isinstance(val, float) and val != val):
            return "N/A"
        try:
            return f"{float(val) * multiply:.{decimals}f}"
        except:
            return "N/A"
    
    prompt = f"""You are a senior equity research analyst. Generate a concise investment memo for {company_data['ticker']} - {company_data['company_name']}.

COMPANY PROFILE
- Sector: {company_data.get('sector', 'N/A')}
- Market Cap: ${safe_value(company_data.get('market_cap'), 2, 1/1e9)}B

QUANTITATIVE SCORES (0-10)
- Alpha Score: {safe_value(company_data.get('overall_alpha_score'))}
- Recommendation: {company_data.get('conviction', 'N/A')}
- Value: {safe_value(company_data.get('value_score'))} | Growth: {safe_value(company_data.get('growth_score'))} | Quality: {safe_value(company_data.get('quality_score'))} | Risk: {safe_value(company_data.get('risk_score'))}

KEY METRICS
- P/E: {safe_value(company_data.get('pe_ratio'))} | ROE: {safe_value(company_data.get('roe'), 1, 100)}% | Rev Growth: {safe_value(company_data.get('revenue_growth_yoy'), 1, 100)}%
- Debt/Equity: {safe_value(company_data.get('debt_to_equity'))}
- Red Flags ({company_data.get('red_flag_count', 0)}): {red_flags_text}

Generate a 3-paragraph investment memo:
1. Investment Thesis (2 sentences)
2. Key Points (4 bullets: 2 strengths, 2 risks)
3. Recommendation (2 sentences)

Be specific and data-driven."""
    return prompt

# Generate memos for all companies
results = []

for idx, company in companies.iterrows():
    company_dict = company.to_dict()
    ticker = company_dict['ticker']
    
    print(f"[{idx+1}/{len(companies)}] Generating memo for {ticker} - {company_dict['company_name']}...")
    
    try:
        # Create prompt
        prompt = create_investment_memo_prompt(company_dict)
        escaped_prompt = prompt.replace("'", "''")
        
        # Call Llama
        query = f"""
        SELECT ai_query(
            '{LLM_MODEL}',
            '{escaped_prompt}'
        ) as investment_memo
        """
        
        result = spark.sql(query).collect()
        memo = result[0]['investment_memo']
        
        # Store result
        results.append({
            'ticker': ticker,
            'company_name': company_dict['company_name'],
            'sector': company_dict['sector'],
            'overall_alpha_score': company_dict['overall_alpha_score'],
            'conviction': company_dict['conviction'],
            'value_score': company_dict['value_score'],
            'growth_score': company_dict['growth_score'],
            'quality_score': company_dict['quality_score'],
            'risk_score': company_dict['risk_score'],
            'red_flag_count': company_dict['red_flag_count'],
            'investment_memo': memo,
            'generation_timestamp': datetime.now()
        })
        
        print(f"   ‚úÖ Generated ({len(memo)} chars)")
        
        # Rate limiting - be nice to the API
        time.sleep(2)
        
    except Exception as e:
        print(f"   ‚ùå Error: {str(e)}")
        results.append({
            'ticker': ticker,
            'company_name': company_dict['company_name'],
            'sector': company_dict['sector'],
            'overall_alpha_score': company_dict['overall_alpha_score'],
            'conviction': company_dict['conviction'],
            'value_score': company_dict.get('value_score'),
            'growth_score': company_dict.get('growth_score'),
            'quality_score': company_dict.get('quality_score'),
            'risk_score': company_dict.get('risk_score'),
            'red_flag_count': company_dict.get('red_flag_count'),
            'investment_memo': f"Error generating memo: {str(e)}",
            'generation_timestamp': datetime.now()
        })

print("\n" + "="*80)
print("üíæ SAVING RESULTS TO DELTA TABLE")
print("="*80)

# Convert to Spark DataFrame
import pandas as pd
results_df = pd.DataFrame(results)
spark_results_df = spark.createDataFrame(results_df)

# Save to Delta table
spark_results_df.write.format("delta").mode("overwrite").saveAsTable("workspace.default.llm_investment_memos")

print(f"\n‚úÖ Saved {len(results)} investment memos to: workspace.default.llm_investment_memos")

# Show summary
print("\nüìä GENERATION SUMMARY:")
print("="*80)

successful = sum(1 for r in results if not r['investment_memo'].startswith('Error'))
print(f"   ‚úÖ Successful: {successful}/{len(results)}")
print(f"   ‚ùå Failed: {len(results) - successful}/{len(results)}")

# Display results table
print("\nüìã Generated Memos:")
display(spark_results_df.select(
    'ticker', 'company_name', 'overall_alpha_score', 'conviction', 
    'investment_memo'
))

print("\n" + "="*80)
print("‚úÖ PHASE 3 COMPLETE - LLM INTEGRATION DONE!")
print("="*80)

print(f"""
üìä Summary:
   ‚úÖ Model: Llama 3.1 405B
   ‚úÖ Memos Generated: {successful}
   ‚úÖ Table: workspace.default.llm_investment_memos
   ‚úÖ Ready for Streamlit Dashboard
""")


# COMMAND ----------

# 1Ô∏è‚É£ Read the Delta table
df = spark.read.format("delta").table("workspace.default.yfin_with_anomalies")

# 2Ô∏è‚É£ Convert to Pandas (optional, for smaller datasets)
pdf = df.toPandas()

# 3Ô∏è‚É£ Save as CSV locally in DBFS
csv_path_dbfs = "yfin_data.csv"
pdf.to_csv(csv_path_dbfs, index=False)

print(f"CSV saved to: {csv_path_dbfs}")


# COMMAND ----------

